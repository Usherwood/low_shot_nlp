{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from collections import Counter\n",
    "import csv\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "repo_dir = cwd.parent\n",
    "dataset_dir = repo_dir / \"datasets\" / \"amazon_movies\"\n",
    "\n",
    "df = pd.read_csv(dataset_dir / \"raw\" / \"en_amazon_movies_0Mto0p5M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_round(x, base=.05, prec=2):\n",
    "    return round(base * round(float(x) / base), prec)\n",
    "\n",
    "\n",
    "def split_dataset_into_even_class_distributions(X_data, Y_data, mini_batch_size=32):\n",
    "    \"\"\"\n",
    "    Split a dataset into train, validate, and test sets that have the same distribution of classes as the original data.\n",
    "    Also this function ensures the resulting datasets are multiples of the mini_batch_size\n",
    "\n",
    "    :param X_data: Numpy 2d array or pandas dataframe, each row is a record\n",
    "    :param Y_data: Pandas Series, the categorial class labels\n",
    "    :param train_size: float 0-1, quantity of the original dataset to be in the train set (val is created from the residual)\n",
    "    :param test_size: float 0-1, quantity of the original dataset to be in the test set (val is created from the residual)\n",
    "    :param mini_batch_size: Int, the size of the mini batches to ensure each dataset is a multiple of that values\n",
    "\n",
    "    :return: 6 data sets of X and Y\n",
    "    \"\"\"\n",
    "\n",
    "    dist = Counter(val for val in Y_data)\n",
    "    print('Total class distribution:', dict(dist))\n",
    "\n",
    "    Y_data.reset_index(drop=True, inplace=True)\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_ix = []\n",
    "    validate_ix = []\n",
    "    test_ix = []\n",
    "    for cls in dist.keys():\n",
    "        num_train = my_round(dist[cls] * train_size, base=mini_batch_size)\n",
    "        num_test = my_round(dist[cls] * test_size, base=mini_batch_size)\n",
    "\n",
    "        cls_targets = Y_data[Y_data == cls]\n",
    "\n",
    "        full_ixs = np.random.choice(cls_targets.index, size=num_train, replace=False)\n",
    "\n",
    "        train_ix += list(full_ixs)\n",
    "\n",
    "        cls_targets = cls_targets[~cls_targets.index.isin(full_ixs)]\n",
    "\n",
    "        full_ixs = np.random.choice(cls_targets.index, size=num_test, replace=False)\n",
    "\n",
    "        test_ix += list(full_ixs)\n",
    "\n",
    "        cls_targets = cls_targets[~cls_targets.index.isin(full_ixs)]\n",
    "\n",
    "        validate_ix += list(cls_targets.index)\n",
    "\n",
    "    X_train = X_data[train_ix]\n",
    "    Y_train = Y_data[train_ix].reset_index(drop=True)\n",
    "\n",
    "    X_validate = X_data[validate_ix]\n",
    "    Y_validate = Y_data[validate_ix].reset_index(drop=True)\n",
    "\n",
    "    X_test = X_data[test_ix]\n",
    "    Y_test = Y_data[test_ix].reset_index(drop=True)\n",
    "\n",
    "    print('Train class distribution:', dict(Counter(val for val in Y_train)))\n",
    "    print('Validate class distribution:', dict(Counter(val for val in Y_validate)))\n",
    "    print('Test class distribution:', dict(Counter(val for val in Y_test)))\n",
    "\n",
    "    return X_train, Y_train, X_validate, Y_validate, X_test, Y_test\n",
    "\n",
    "\n",
    "def parse_df_into_sets_by_tertiary_grouping(df, ter=\"ID\", y=\"Score\", x=\"text\", train_prop=.7, test_prop=.2):\n",
    "\n",
    "    ideal_train_size = math.floor(len(df)*train_prop)\n",
    "    ideal_test_size = math.floor(len(df)*test_prop)\n",
    "    ideal_val_size = len(df) - (ideal_train_size + ideal_test_size)\n",
    "\n",
    "    title_avg_count = df[ter].value_counts().mean()\n",
    "\n",
    "    titles = df[ter].unique().tolist()\n",
    "\n",
    "    train_titles = []\n",
    "    train_count = 0\n",
    "\n",
    "    while train_count <= ideal_train_size - title_avg_count:\n",
    "        title = np.random.choice(titles)\n",
    "        titles.remove(title)\n",
    "        train_titles.append(title)\n",
    "        train_count += len(df[df[ter] == title])\n",
    "\n",
    "    test_titles = []\n",
    "    test_count = 0\n",
    "\n",
    "    while test_count <= ideal_test_size - title_avg_count:\n",
    "        title = np.random.choice(titles)\n",
    "        titles.remove(title)\n",
    "        test_titles.append(title)\n",
    "        test_count += len(df[df[ter] == title]) \n",
    "\n",
    "    val_titles = titles\n",
    "    val_count = len(df[df[ter].isin(val_titles)])\n",
    "\n",
    "    print(\"Ideal train size is:\", ideal_train_size, \"whearas the actual is:\", train_count)\n",
    "    print(\"Ideal val size is:\", ideal_val_size, \"whearas the actual is:\", val_count)\n",
    "    print(\"Ideal test size is:\", ideal_test_size, \"whearas the actual is:\", test_count)\n",
    "\n",
    "    df_trn = df[df[ter].isin(train_titles)]\n",
    "    df_val = df[df[ter].isin(val_titles)]\n",
    "    df_tst = df[df[ter].isin(test_titles)]\n",
    "    \n",
    "    print(\"Train class distribution:\", df_trn[y].value_counts())\n",
    "    print(\"Val class distribution:\", df_val[y].value_counts())\n",
    "    print(\"Test class distribution:\", df_tst[y].value_counts())\n",
    "\n",
    "    return df_trn[x], df_trn[y], df_val[x], df_val[y], df_tst[x], df_tst[y]\n",
    "\n",
    "def balance_classes(X_data, Y_data, min_class_value=None):\n",
    "    \n",
    "    kes = Y_data.value_counts().keys().tolist()\n",
    "    \n",
    "    min_class = Y_data.value_counts().keys()[-1]\n",
    "    \n",
    "    if not min_class_value:\n",
    "        min_class_value = Y_data.value_counts().values[-1]\n",
    "    \n",
    "    all_ix = []\n",
    "            \n",
    "    for ke in kes:\n",
    "        ke_indexes = Y_data[Y_data == ke].index.tolist()\n",
    "        if len(ke_indexes) == min_class_value:\n",
    "            all_ix += ke_indexes\n",
    "        else:\n",
    "            all_ix += list(np.random.choice(ke_indexes, size=min_class_value, replace=False))\n",
    "        \n",
    "    X_data = X_data[all_ix]\n",
    "    Y_data = Y_data[all_ix]\n",
    "    \n",
    "    print(\"Y class distribution:\", Y_data.value_counts())\n",
    "    \n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>User ID</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Helpfulness</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Header</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>At Mentions</th>\n",
       "      <th>Extracted URLs</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A141HP4LYPWMSR</td>\n",
       "      <td>Brian E. Erland \"Rainbow Sphinx\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1182729600</td>\n",
       "      <td>\"There Is So Much Darkness Now ~ Come For The ...</td>\n",
       "      <td>Synopsis: On the daily trek from Juarez, Mexic...</td>\n",
       "      <td>synopsis on the daily trek from juarez mexico ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>synopsi on the daili trek from juarez mexico t...</td>\n",
       "      <td>synopsi daili trek juarez mexico el paso texa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>Grady Harp</td>\n",
       "      <td>4/4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1181952000</td>\n",
       "      <td>Worthwhile and Important Story Hampered by Poo...</td>\n",
       "      <td>THE VIRGIN OF JUAREZ is based on true events s...</td>\n",
       "      <td>the virgin of juarez is based on true events s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>the virgin of juarez is base on true event sur...</td>\n",
       "      <td>virgin juarez base true event surround crime p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A1I7QGUDP043DG</td>\n",
       "      <td>Chrissy K. McVay \"Writer\"</td>\n",
       "      <td>8/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1164844800</td>\n",
       "      <td>This movie needed to be made.</td>\n",
       "      <td>The scenes in this film can be very disquietin...</td>\n",
       "      <td>the scenes in this film can be very disquietin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>the scene in this film can be veri disquiet du...</td>\n",
       "      <td>scene film veri disquiet due graphic enact rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A1M5405JH9THP9</td>\n",
       "      <td>golgotha.gov</td>\n",
       "      <td>1/1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1197158400</td>\n",
       "      <td>distantly based on a real tragedy</td>\n",
       "      <td>THE VIRGIN OF JUAREZ (2006)&lt;br /&gt;directed by K...</td>\n",
       "      <td>the virgin of juarez 2006 br directed by kevin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>the virgin of juarez 2006 br direct by kevin j...</td>\n",
       "      <td>virgin juarez 2006 br direct kevin jame dobson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>ATXL536YX71TR</td>\n",
       "      <td>KerrLines \"&amp;#34;Movies,Music,Theatre&amp;#34;\"</td>\n",
       "      <td>1/1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1188345600</td>\n",
       "      <td>\"What's going on down in Juarez and shining a ...</td>\n",
       "      <td>Informationally, this SHOWTIME original is ess...</td>\n",
       "      <td>informationally this showtime original is esse...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>inform this showtim origin is essenti view for...</td>\n",
       "      <td>inform showtim origin essenti view enlighten a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         User ID                                   User Name  \\\n",
       "0  B003AI2VGA  A141HP4LYPWMSR            Brian E. Erland \"Rainbow Sphinx\"   \n",
       "1  B003AI2VGA  A328S9RN3U5M68                                  Grady Harp   \n",
       "2  B003AI2VGA  A1I7QGUDP043DG                   Chrissy K. McVay \"Writer\"   \n",
       "3  B003AI2VGA  A1M5405JH9THP9                                golgotha.gov   \n",
       "4  B003AI2VGA   ATXL536YX71TR  KerrLines \"&#34;Movies,Music,Theatre&#34;\"   \n",
       "\n",
       "  Helpfulness  Score        Time  \\\n",
       "0         7/7    3.0  1182729600   \n",
       "1         4/4    3.0  1181952000   \n",
       "2        8/10    5.0  1164844800   \n",
       "3         1/1    3.0  1197158400   \n",
       "4         1/1    3.0  1188345600   \n",
       "\n",
       "                                              Header  \\\n",
       "0  \"There Is So Much Darkness Now ~ Come For The ...   \n",
       "1  Worthwhile and Important Story Hampered by Poo...   \n",
       "2                      This movie needed to be made.   \n",
       "3                  distantly based on a real tragedy   \n",
       "4  \"What's going on down in Juarez and shining a ...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Synopsis: On the daily trek from Juarez, Mexic...   \n",
       "1  THE VIRGIN OF JUAREZ is based on true events s...   \n",
       "2  The scenes in this film can be very disquietin...   \n",
       "3  THE VIRGIN OF JUAREZ (2006)<br />directed by K...   \n",
       "4  Informationally, this SHOWTIME original is ess...   \n",
       "\n",
       "                                             Cleaned Hashtags At Mentions  \\\n",
       "0  synopsis on the daily trek from juarez mexico ...       []          []   \n",
       "1  the virgin of juarez is based on true events s...       []          []   \n",
       "2  the scenes in this film can be very disquietin...       []          []   \n",
       "3  the virgin of juarez 2006 br directed by kevin...       []          []   \n",
       "4  informationally this showtime original is esse...       []          []   \n",
       "\n",
       "  Extracted URLs                                            Stemmed  \\\n",
       "0             []  synopsi on the daili trek from juarez mexico t...   \n",
       "1             []  the virgin of juarez is base on true event sur...   \n",
       "2             []  the scene in this film can be veri disquiet du...   \n",
       "3             []  the virgin of juarez 2006 br direct by kevin j...   \n",
       "4             []  inform this showtim origin is essenti view for...   \n",
       "\n",
       "                                        Preprocessed  \n",
       "0  synopsi daili trek juarez mexico el paso texa ...  \n",
       "1  virgin juarez base true event surround crime p...  \n",
       "2  scene film veri disquiet due graphic enact rea...  \n",
       "3  virgin juarez 2006 br direct kevin jame dobson...  \n",
       "4  inform showtim origin essenti view enlighten a...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Unfair\" - same movie mixed in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = split_dataset_into_even_class_distributions(X_data=df['Text'],\n",
    "                                                                                                       Y_data=df['Score'], \n",
    "                                                                                                       train_size=.7, \n",
    "                                                                                                       test_size=.2,\n",
    "                                                                                                       mini_batch_size=1)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_movies/unbalanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_movies/unbalanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_movies/unbalanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced\n",
    "\n",
    "lowest_class_num = min(df[\"Score\"].value_counts())\n",
    "print(\"Smallest class size: {}\".format(lowest_class_num))\n",
    "df_bl = df.groupby(\"Score\").apply(lambda x: x.sample(n=lowest_class_num, replace=False)).reset_index(drop=True).loc[:, [\"Text\", \"Score\"]]\n",
    "df_bl = df_bl.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = split_dataset_into_even_class_distributions(X_data=df_bl['Text'],\n",
    "                                                                                                       Y_data=df_bl['Score'], \n",
    "                                                                                                       train_size=.7, \n",
    "                                                                                                       test_size=.2,\n",
    "                                                                                                       mini_batch_size=1)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_movies/balanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_movies/balanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_movies/balanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"ID\",\n",
    "                                                                                                   y=\"Score\",\n",
    "                                                                                                   x=\"Text\",\n",
    "                                                                                                   train_prop=.7, \n",
    "                                                                                                   test_prop=.2)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_movies/fair_unbalanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_movies/fair_unbalanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_movies/fair_unbalanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"ID\",\n",
    "                                                                                                   y=\"Score\",\n",
    "                                                                                                   x=\"Text\",\n",
    "                                                                                                   train_prop=.7, \n",
    "                                                                                                   test_prop=.2)\n",
    "\n",
    "X_train, Y_train = balance_classes(X_data=X_train, Y_data=Y_train)\n",
    "X_validate, Y_validate = balance_classes(X_data=X_validate, Y_data=Y_validate)\n",
    "X_test, Y_test = balance_classes(X_data=X_test, Y_data=Y_test)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_movies/fair_balanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_movies/fair_balanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_movies/fair_balanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair split for Low Shot paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal train size is: 200000 whearas the actual is: 199971\n",
      "Ideal val size is: 100000 whearas the actual is: 100019\n",
      "Ideal test size is: 200000 whearas the actual is: 200010\n",
      "Train class distribution: 5.0    108800\n",
      "4.0     41601\n",
      "3.0     20198\n",
      "1.0     17263\n",
      "2.0     12109\n",
      "Name: Score, dtype: int64\n",
      "Val class distribution: 5.0    54982\n",
      "4.0    21283\n",
      "3.0    10126\n",
      "1.0     7805\n",
      "2.0     5823\n",
      "Name: Score, dtype: int64\n",
      "Test class distribution: 5.0    111157\n",
      "4.0     41631\n",
      "3.0     19950\n",
      "1.0     15758\n",
      "2.0     11514\n",
      "Name: Score, dtype: int64\n",
      "5 classes\n"
     ]
    }
   ],
   "source": [
    "# Unbalanced\n",
    "\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"ID\",\n",
    "                                                                                                   y=\"Score\",\n",
    "                                                                                                   x=\"Text\",\n",
    "                                                                                                   train_prop=.4, \n",
    "                                                                                                   test_prop=.4)\n",
    "\n",
    "N_CLASSES = len(Y_train.value_counts())\n",
    "print(N_CLASSES, \"classes\")\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "if len(df_val) > 25000:\n",
    "    df_val = df_val.sample(n=25000)\n",
    "\n",
    "df_tst = df_tst.sample(n=25000)\n",
    "\n",
    "for size in [100, 300, 1000]:\n",
    "    df_trn.sample(n=size*N_CLASSES).to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_val.to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_tst.to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal train size is: 200000 whearas the actual is: 199970\n",
      "Ideal val size is: 100000 whearas the actual is: 100043\n",
      "Ideal test size is: 200000 whearas the actual is: 199987\n",
      "Train class distribution: 5.0    108737\n",
      "4.0     42130\n",
      "3.0     20294\n",
      "1.0     16895\n",
      "2.0     11914\n",
      "Name: Score, dtype: int64\n",
      "Val class distribution: 5.0    55981\n",
      "4.0    20364\n",
      "3.0     9767\n",
      "1.0     8075\n",
      "2.0     5856\n",
      "Name: Score, dtype: int64\n",
      "Test class distribution: 5.0    110221\n",
      "4.0     42021\n",
      "3.0     20213\n",
      "1.0     15856\n",
      "2.0     11676\n",
      "Name: Score, dtype: int64\n",
      "Y class distribution: 2.0    5000\n",
      "1.0    5000\n",
      "3.0    5000\n",
      "4.0    5000\n",
      "5.0    5000\n",
      "Name: Score, dtype: int64\n",
      "Y class distribution: 2.0    5000\n",
      "1.0    5000\n",
      "3.0    5000\n",
      "4.0    5000\n",
      "5.0    5000\n",
      "Name: Score, dtype: int64\n",
      "Y class distribution: 2.0    100\n",
      "1.0    100\n",
      "3.0    100\n",
      "4.0    100\n",
      "5.0    100\n",
      "Name: Score, dtype: int64\n",
      "Y class distribution: 2.0    300\n",
      "1.0    300\n",
      "3.0    300\n",
      "4.0    300\n",
      "5.0    300\n",
      "Name: Score, dtype: int64\n",
      "Y class distribution: 2.0    1000\n",
      "1.0    1000\n",
      "3.0    1000\n",
      "4.0    1000\n",
      "5.0    1000\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Balanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"ID\",\n",
    "                                                                                                   y=\"Score\",\n",
    "                                                                                                   x=\"Text\",\n",
    "                                                                                                   train_prop=.4, \n",
    "                                                                                                   test_prop=.4)\n",
    "\n",
    "X_validate, Y_validate = balance_classes(X_data=X_validate, Y_data=Y_validate, min_class_value=5000)\n",
    "X_test, Y_test = balance_classes(X_data=X_test, Y_data=Y_test, min_class_value=5000)\n",
    "\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "for size in [100, 300, 1000]:\n",
    "    X_train_crnt, Y_train_crnt = balance_classes(X_data=X_train, Y_data=Y_train, min_class_value=size)\n",
    "    df_trn = pd.DataFrame(np.array([Y_train_crnt, X_train_crnt]).T, columns=['label', 'text'])\n",
    "    df_trn.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_val.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_tst.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    100\n",
       "1.0    100\n",
       "3.0    100\n",
       "4.0    100\n",
       "5.0    100\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir / (\"fair_balanced_\"+str(100)) / \"train.csv\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
