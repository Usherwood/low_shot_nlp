{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from collections import Counter\n",
    "import csv\n",
    "import math\n",
    "import gzip\n",
    "import pathlib\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path, max_rows=100000):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for i, l in enumerate(g):\n",
    "        if i == max_rows:\n",
    "            return eval(l)\n",
    "        yield eval(l)\n",
    "\n",
    "def get_df(path, max_rows=100000):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path, max_rows):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "repo_dir = cwd.parent\n",
    "dataset_dir = repo_dir / \"datasets\" / \"amazon_books\"\n",
    "\n",
    "df = get_df(dataset_dir / 'reviews_Books_5.json.gz', max_rows=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_round(x, base=.05, prec=2):\n",
    "    return round(base * round(float(x) / base), prec)\n",
    "\n",
    "\n",
    "def split_dataset_into_even_class_distributions(X_data, Y_data, mini_batch_size=32):\n",
    "    \"\"\"\n",
    "    Split a dataset into train, validate, and test sets that have the same distribution of classes as the original data.\n",
    "    Also this function ensures the resulting datasets are multiples of the mini_batch_size\n",
    "\n",
    "    :param X_data: Numpy 2d array or pandas dataframe, each row is a record\n",
    "    :param Y_data: Pandas Series, the categorial class labels\n",
    "    :param train_size: float 0-1, quantity of the original dataset to be in the train set (val is created from the residual)\n",
    "    :param test_size: float 0-1, quantity of the original dataset to be in the test set (val is created from the residual)\n",
    "    :param mini_batch_size: Int, the size of the mini batches to ensure each dataset is a multiple of that values\n",
    "\n",
    "    :return: 6 data sets of X and Y\n",
    "    \"\"\"\n",
    "\n",
    "    dist = Counter(val for val in Y_data)\n",
    "    print('Total class distribution:', dict(dist))\n",
    "\n",
    "    Y_data.reset_index(drop=True, inplace=True)\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_ix = []\n",
    "    validate_ix = []\n",
    "    test_ix = []\n",
    "    for cls in dist.keys():\n",
    "        num_train = my_round(dist[cls] * train_size, base=mini_batch_size)\n",
    "        num_test = my_round(dist[cls] * test_size, base=mini_batch_size)\n",
    "\n",
    "        cls_targets = Y_data[Y_data == cls]\n",
    "\n",
    "        full_ixs = np.random.choice(cls_targets.index, size=num_train, replace=False)\n",
    "\n",
    "        train_ix += list(full_ixs)\n",
    "\n",
    "        cls_targets = cls_targets[~cls_targets.index.isin(full_ixs)]\n",
    "\n",
    "        full_ixs = np.random.choice(cls_targets.index, size=num_test, replace=False)\n",
    "\n",
    "        test_ix += list(full_ixs)\n",
    "\n",
    "        cls_targets = cls_targets[~cls_targets.index.isin(full_ixs)]\n",
    "\n",
    "        validate_ix += list(cls_targets.index)\n",
    "\n",
    "    X_train = X_data[train_ix]\n",
    "    Y_train = Y_data[train_ix].reset_index(drop=True)\n",
    "\n",
    "    X_validate = X_data[validate_ix]\n",
    "    Y_validate = Y_data[validate_ix].reset_index(drop=True)\n",
    "\n",
    "    X_test = X_data[test_ix]\n",
    "    Y_test = Y_data[test_ix].reset_index(drop=True)\n",
    "\n",
    "    print('Train class distribution:', dict(Counter(val for val in Y_train)))\n",
    "    print('Validate class distribution:', dict(Counter(val for val in Y_validate)))\n",
    "    print('Test class distribution:', dict(Counter(val for val in Y_test)))\n",
    "\n",
    "    return X_train, Y_train, X_validate, Y_validate, X_test, Y_test\n",
    "\n",
    "\n",
    "def parse_df_into_sets_by_tertiary_grouping(df, ter=\"ID\", y=\"Score\", x=\"text\", train_prop=.7, test_prop=.2):\n",
    "\n",
    "    ideal_train_size = math.floor(len(df)*train_prop)\n",
    "    ideal_test_size = math.floor(len(df)*test_prop)\n",
    "    ideal_val_size = len(df) - (ideal_train_size + ideal_test_size)\n",
    "\n",
    "    title_avg_count = df[ter].value_counts().mean()\n",
    "\n",
    "    titles = df[ter].unique().tolist()\n",
    "\n",
    "    train_titles = []\n",
    "    train_count = 0\n",
    "\n",
    "    while train_count <= ideal_train_size - title_avg_count:\n",
    "        title = np.random.choice(titles)\n",
    "        titles.remove(title)\n",
    "        train_titles.append(title)\n",
    "        train_count += len(df[df[ter] == title])\n",
    "\n",
    "    test_titles = []\n",
    "    test_count = 0\n",
    "\n",
    "    while test_count <= ideal_test_size - title_avg_count:\n",
    "        title = np.random.choice(titles)\n",
    "        titles.remove(title)\n",
    "        test_titles.append(title)\n",
    "        test_count += len(df[df[ter] == title]) \n",
    "\n",
    "    val_titles = titles\n",
    "    val_count = len(df[df[ter].isin(val_titles)])\n",
    "\n",
    "    print(\"Ideal train size is:\", ideal_train_size, \"whearas the actual is:\", train_count)\n",
    "    print(\"Ideal val size is:\", ideal_val_size, \"whearas the actual is:\", val_count)\n",
    "    print(\"Ideal test size is:\", ideal_test_size, \"whearas the actual is:\", test_count)\n",
    "\n",
    "    df_trn = df[df[ter].isin(train_titles)]\n",
    "    df_val = df[df[ter].isin(val_titles)]\n",
    "    df_tst = df[df[ter].isin(test_titles)]\n",
    "    \n",
    "    print(\"Train class distribution:\", df_trn[y].value_counts())\n",
    "    print(\"Val class distribution:\", df_val[y].value_counts())\n",
    "    print(\"Test class distribution:\", df_tst[y].value_counts())\n",
    "\n",
    "    return df_trn[x], df_trn[y], df_val[x], df_val[y], df_tst[x], df_tst[y]\n",
    "\n",
    "def balance_classes(X_data, Y_data, min_class_value=None):\n",
    "    \n",
    "    kes = Y_data.value_counts().keys().tolist()\n",
    "    \n",
    "    min_class = Y_data.value_counts().keys()[-1]\n",
    "    \n",
    "    if not min_class_value:\n",
    "        min_class_value = Y_data.value_counts().values[-1]\n",
    "    \n",
    "    all_ix = []\n",
    "            \n",
    "    for ke in kes:\n",
    "        ke_indexes = Y_data[Y_data == ke].index.tolist()\n",
    "        if len(ke_indexes) == min_class_value:\n",
    "            all_ix += ke_indexes\n",
    "        else:\n",
    "            all_ix += list(np.random.choice(ke_indexes, size=min_class_value, replace=False))\n",
    "        \n",
    "    X_data = X_data[all_ix]\n",
    "    Y_data = Y_data[all_ix]\n",
    "    \n",
    "    print(\"Y class distribution:\", Y_data.value_counts())\n",
    "    \n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"asin\",\n",
    "                                                                                                   y=\"overall\",\n",
    "                                                                                                   x=\"reviewText\",\n",
    "                                                                                                   train_prop=.7, \n",
    "                                                                                                   test_prop=.2)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_books/fair_unbalanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_books/fair_unbalanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_books/fair_unbalanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"asin\",\n",
    "                                                                                                   y=\"overall\",\n",
    "                                                                                                   x=\"reviewText\",\n",
    "                                                                                                   train_prop=.7, \n",
    "                                                                                                   test_prop=.2)\n",
    "\n",
    "X_train, Y_train = balance_classes(X_data=X_train, Y_data=Y_train)\n",
    "X_validate, Y_validate = balance_classes(X_data=X_validate, Y_data=Y_validate)\n",
    "X_test, Y_test = balance_classes(X_data=X_test, Y_data=Y_test)\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "df_trn.to_csv(\"../datasets/amazon_books/fair_balanced/train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_val.to_csv(\"../datasets/amazon_books/fair_balanced/validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "df_tst.to_csv(\"../datasets/amazon_books/fair_balanced/test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced\n",
    "\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"asin\",\n",
    "                                                                                                   y=\"overall\",\n",
    "                                                                                                   x=\"reviewText\",\n",
    "                                                                                                   train_prop=.4, \n",
    "                                                                                                   test_prop=.4)\n",
    "\n",
    "N_CLASSES = len(Y_train.value_counts())\n",
    "print(N_CLASSES, \"classes\")\n",
    "\n",
    "df_trn = pd.DataFrame(np.array([Y_train, X_train]).T, columns=['label', 'text'])\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "if len(df_val) > 25000:\n",
    "    df_val = df_val.sample(n=25000)\n",
    "\n",
    "df_tst = df_tst.sample(n=25000)\n",
    "\n",
    "for size in [100, 300, 1000]:\n",
    "    df_trn.sample(n=size*N_CLASSES).to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_val.to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_tst.to_csv(dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced\n",
    "X_train, Y_train, X_validate, Y_validate, X_test, Y_test = parse_df_into_sets_by_tertiary_grouping(df=df,\n",
    "                                                                                                   ter=\"asin\",\n",
    "                                                                                                   y=\"overall\",\n",
    "                                                                                                   x=\"reviewText\",\n",
    "                                                                                                   train_prop=.4, \n",
    "                                                                                                   test_prop=.4)\n",
    "\n",
    "X_validate, Y_validate = balance_classes(X_data=X_validate, Y_data=Y_validate)\n",
    "X_test, Y_test = balance_classes(X_data=X_test, Y_data=Y_test, min_class_value=5000)\n",
    "\n",
    "df_val = pd.DataFrame(np.array([Y_validate, X_validate]).T, columns=['label', 'text'])\n",
    "df_tst = pd.DataFrame(np.array([Y_test, X_test]).T, columns=['label', 'text'])\n",
    "\n",
    "for size in [100, 300, 1000]:\n",
    "    X_train_crnt, Y_train_crnt = balance_classes(X_data=X_train, Y_data=Y_train, min_class_value=size)\n",
    "    df_trn = pd.DataFrame(np.array([Y_train_crnt, X_train_crnt]).T, columns=['label', 'text'])\n",
    "    df_trn.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"train.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_val.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"validate.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    df_tst.to_csv(dataset_dir / (\"fair_balanced_\"+str(size)) / \"test.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir / (\"fair_unbalanced_\"+str(size)) / \"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir / (\"fair_balanced_\"+str(100)) / \"train.csv\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
